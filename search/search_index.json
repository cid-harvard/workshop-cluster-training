{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Effective Cluster Usage and Data Management \u00b6 Thursday. Sep 15, 2022. Pre-requisites \u00b6 Please perform the following steps before the workshop , in the interest of time: Pre-requisites Workshop Topics \u00b6 Project folder structures and data management Data management for GL Planning for a new project Folder structures Best practices and documentation Cluster usage Basic terminal usage How to submit jobs on the cluster and manage them Using Python / Stata remotely Tips / tricks: How to write code that other people can run How to run Python / R / Stata code on the cluster","title":"Home"},{"location":"#effective-cluster-usage-and-data-management","text":"Thursday. Sep 15, 2022.","title":"Effective Cluster Usage and Data Management"},{"location":"#pre-requisites","text":"Please perform the following steps before the workshop , in the interest of time: Pre-requisites","title":"Pre-requisites"},{"location":"#workshop-topics","text":"Project folder structures and data management Data management for GL Planning for a new project Folder structures Best practices and documentation Cluster usage Basic terminal usage How to submit jobs on the cluster and manage them Using Python / Stata remotely Tips / tricks: How to write code that other people can run How to run Python / R / Stata code on the cluster","title":"Workshop Topics"},{"location":"cluster_usage/fasrc_advanced_cluster_usage/","text":"Working with Linux \u00b6 The FASRC cluster runs on Linux. This section discusses some commands that might be useful for everday cluster usage through the terminal: Change directory: cd <path to directory> Create new folder in current directory: mkdir <name of new folder> List contents of current directory: ls Including hidden files: ls -al With file sizes in KB/MB/GB instead of bytes: ls -alh Create an empty file: touch <filename> Open a Vim text editor: vi <filename> (There are a bunch of vim specific commands you need to learn to use Vim) Output the contents of a text file: cat <filename> Output the first few lines of a file: head <filename> Copy file: cp <old_file_path> <new_file_path> Move file: mv <old_file_path> <new_file_path> Rename file: mv <old_file_name> <new_file_name> Remove file: rm <file_name> For more commonly used commands, here's a cheatsheet SLURM commands \u00b6 # Adapted from https://vsoch.github.io/lessons/sherlock-jobs/ # Show the overall status of each partition sinfo # Submit a job sbatch jobFile.job # See the entire job queue squeue # See only jobs for a given user squeue -u username # Count number of running / in queue jobs squeue -u username | wc -l # Get estimated start times for your jobs (when Sherlock is busy) squeue --start -u username # Show the status of a currently running job sstat -j jobID # Show the final status of a finished job sacct -j jobID # Kill a job with ID $PID scancel $PID # Kill ALL jobs for a user scancel -u username # Kill all pending jobs scancel -u username --state = pending # Run interactive node with 16 cores (12 plus all memory on 1 node) for 4 hours: srun -n 12 -N 1 --mem = 64000 --time 4 :0:0 --pty bash # Claim interactive node for exclusive use, 8 hours srun --exclusive --time 8 :0:0 --pty bash # Same as above, but with X11 forwarding srun --exclusive --time 8 :0:0 --x11 --pty bash # Same as above, but with priority over your other jobs srun --nice = 9999 --exclusive --time 8 :0:0 --x11 --pty -p dev -t 12 :00 bash # Check utilization of group allocation sacct # Running jobs in the group allocation srun -p groupid sbatch -p groupid # Stop/restart jobs interactively # To stop: scancel -s SIGSTOP job id # To restart (this won't free up memory): scancel -s SIGCONT job id # Get usage for file systems df -k df -h $HOME # Get usage for your home directory du # Counting Files find . -type f | wc -l # Check lab disk quota # Based on https://docs.rc.fas.harvard.edu/kb/faq/ lfs quota -hg hausmann_lab /n/holystore01 lfs quota -hg hausmann_lab /n/holylfs05 Tips for quickly connecting to FASRC (Mac or Linux only) \u00b6 Set up custom command sshody to ssh from laptop to RC: aliased to ssh -o ServerAliveInterval = 30 -CY username@login.rc.fas.harvard.edu For new connections, you can then do ssh ody to piggyback off the first connection using ControlMaster For piggybacking, the following settings need to be enabled in ~/.ssh/config : Host * AddKeysToAgent yes UseKeychain yes IdentityFile ~/.ssh/id_rsa XAuthLocation /opt/X11/bin/xauth Host login.rc.fas.harvard.edu odyssey ody HostName login.rc.fas.harvard.edu User shreyasgm ControlMaster auto ControlPath ~/.ssh/%r@%h:%p ForwardAgent yes ForwardX11Trusted yes","title":"Advanced usage"},{"location":"cluster_usage/fasrc_advanced_cluster_usage/#working-with-linux","text":"The FASRC cluster runs on Linux. This section discusses some commands that might be useful for everday cluster usage through the terminal: Change directory: cd <path to directory> Create new folder in current directory: mkdir <name of new folder> List contents of current directory: ls Including hidden files: ls -al With file sizes in KB/MB/GB instead of bytes: ls -alh Create an empty file: touch <filename> Open a Vim text editor: vi <filename> (There are a bunch of vim specific commands you need to learn to use Vim) Output the contents of a text file: cat <filename> Output the first few lines of a file: head <filename> Copy file: cp <old_file_path> <new_file_path> Move file: mv <old_file_path> <new_file_path> Rename file: mv <old_file_name> <new_file_name> Remove file: rm <file_name> For more commonly used commands, here's a cheatsheet","title":"Working with Linux"},{"location":"cluster_usage/fasrc_advanced_cluster_usage/#slurm-commands","text":"# Adapted from https://vsoch.github.io/lessons/sherlock-jobs/ # Show the overall status of each partition sinfo # Submit a job sbatch jobFile.job # See the entire job queue squeue # See only jobs for a given user squeue -u username # Count number of running / in queue jobs squeue -u username | wc -l # Get estimated start times for your jobs (when Sherlock is busy) squeue --start -u username # Show the status of a currently running job sstat -j jobID # Show the final status of a finished job sacct -j jobID # Kill a job with ID $PID scancel $PID # Kill ALL jobs for a user scancel -u username # Kill all pending jobs scancel -u username --state = pending # Run interactive node with 16 cores (12 plus all memory on 1 node) for 4 hours: srun -n 12 -N 1 --mem = 64000 --time 4 :0:0 --pty bash # Claim interactive node for exclusive use, 8 hours srun --exclusive --time 8 :0:0 --pty bash # Same as above, but with X11 forwarding srun --exclusive --time 8 :0:0 --x11 --pty bash # Same as above, but with priority over your other jobs srun --nice = 9999 --exclusive --time 8 :0:0 --x11 --pty -p dev -t 12 :00 bash # Check utilization of group allocation sacct # Running jobs in the group allocation srun -p groupid sbatch -p groupid # Stop/restart jobs interactively # To stop: scancel -s SIGSTOP job id # To restart (this won't free up memory): scancel -s SIGCONT job id # Get usage for file systems df -k df -h $HOME # Get usage for your home directory du # Counting Files find . -type f | wc -l # Check lab disk quota # Based on https://docs.rc.fas.harvard.edu/kb/faq/ lfs quota -hg hausmann_lab /n/holystore01 lfs quota -hg hausmann_lab /n/holylfs05","title":"SLURM commands"},{"location":"cluster_usage/fasrc_advanced_cluster_usage/#tips-for-quickly-connecting-to-fasrc-mac-or-linux-only","text":"Set up custom command sshody to ssh from laptop to RC: aliased to ssh -o ServerAliveInterval = 30 -CY username@login.rc.fas.harvard.edu For new connections, you can then do ssh ody to piggyback off the first connection using ControlMaster For piggybacking, the following settings need to be enabled in ~/.ssh/config : Host * AddKeysToAgent yes UseKeychain yes IdentityFile ~/.ssh/id_rsa XAuthLocation /opt/X11/bin/xauth Host login.rc.fas.harvard.edu odyssey ody HostName login.rc.fas.harvard.edu User shreyasgm ControlMaster auto ControlPath ~/.ssh/%r@%h:%p ForwardAgent yes ForwardX11Trusted yes","title":"Tips for quickly connecting to FASRC (Mac or Linux only)"},{"location":"cluster_usage/fasrc_basic_cluster_usage/","text":"FASRC documentation This page is intended to provide a basic overview of how to use the FASRC cluster. A quickstart guide is available on FASRC's documentation page. For more detailed information, please see the FASRC User Guide . Introduction to FASRC \u00b6 A compute cluster (used to be called supercomputers) is essentially a set of computers tied together, allowing for larger scale / shared usage. The FASRC cluster is a shared resource for the Harvard community, managed by the FAS Research Computing group. The RCE cluster is shutting down entirely on November 1st, 2022, and all users and projects are being migrated to FASRC. Note that the RCE support teams from IQSS will continue to be the Growth Lab's point of contact for FASRC support requests. Pre-requisites \u00b6 You should have a Harvard ID, and have an account on FASRC. If you don't have an account on FASRC yet, please sign up here and request access to hausmann_lab . Email Andrea Hayes for approval once you're done with these steps. You should have the FASRC VPN setup You should have two-factor authentication setup for your FASRC account. For GL users who work with confidential data (almost all applied projects do), you should have completed the necessary research data security trainings. Link to the university-wide training is here . Users who work with human subjects data need additional training certifications. Please contact Andrea Hayes for more information. (Optional) If you plan to work on Python, install VSCode and Miniconda . (Optional) Install the Remote SSH extension and the Python extension for VSCode. Easy installation for Mac users using Homebrew # Make sure XCode is installed sudo xcode-select --install # Install Homebrew /bin/bash -c \" $( curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install.sh ) \" # Check Homebrew installation brew doctor # Install VSCode brew install --cask visual-studio-code # Install Miniconda brew install --cask miniconda # Set paths echo 'export PATH=/usr/local/anaconda3/bin:$PATH' >> ~/.bash_profile echo 'export PATH=/opt/homebrew/anaconda3/bin:$PATH' >> ~/.bash_profile source ~/.bash_profile Jargon \u00b6 Note that these are not the exact technical definitions of these terms, but my (potentially simplistic) understanding of them. FASRC = FAS Research Computing Cannon = Cluster for level 2 data. This is where most of the academic group's data lives. FASSE = FAS Secure Environment. Cluster for level 3 data. This is where the applied projects' data will live. SLURM = a job scheduler that is used to manage jobs on the cluster. This is the scheduler that FASRC uses. Partition = A portion of the cluster that is reserved for a specific purpose. For example, the gpu partition is reserved for GPU jobs. Job = A task that is submitted to the cluster. A job can be a single task, or a series of tasks that are run in sequence. Node = an individual server. Core = a single CPU. Storage \u00b6 FASRC gives every user 100GB of storage space in your home folder. In addition, the Growth Lab has lab storage available at the following locations: - /n/hausmann_lab/lab/ - /n/holylfs05/LABS/hausmann_lab Applied projects will be moved to FASSE. The GL data committee has submitted requests to FASRC to move RCE projects to FASSE, and the requests are being considered on an individual basis by FASRC for data security review. Working with VDI \u00b6 Great guides available directly on FASRC documentation page: Using VDI VDI apps FASSE VDI apps","title":"Basic usage"},{"location":"cluster_usage/fasrc_basic_cluster_usage/#introduction-to-fasrc","text":"A compute cluster (used to be called supercomputers) is essentially a set of computers tied together, allowing for larger scale / shared usage. The FASRC cluster is a shared resource for the Harvard community, managed by the FAS Research Computing group. The RCE cluster is shutting down entirely on November 1st, 2022, and all users and projects are being migrated to FASRC. Note that the RCE support teams from IQSS will continue to be the Growth Lab's point of contact for FASRC support requests.","title":"Introduction to FASRC"},{"location":"cluster_usage/fasrc_basic_cluster_usage/#pre-requisites","text":"You should have a Harvard ID, and have an account on FASRC. If you don't have an account on FASRC yet, please sign up here and request access to hausmann_lab . Email Andrea Hayes for approval once you're done with these steps. You should have the FASRC VPN setup You should have two-factor authentication setup for your FASRC account. For GL users who work with confidential data (almost all applied projects do), you should have completed the necessary research data security trainings. Link to the university-wide training is here . Users who work with human subjects data need additional training certifications. Please contact Andrea Hayes for more information. (Optional) If you plan to work on Python, install VSCode and Miniconda . (Optional) Install the Remote SSH extension and the Python extension for VSCode. Easy installation for Mac users using Homebrew # Make sure XCode is installed sudo xcode-select --install # Install Homebrew /bin/bash -c \" $( curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install.sh ) \" # Check Homebrew installation brew doctor # Install VSCode brew install --cask visual-studio-code # Install Miniconda brew install --cask miniconda # Set paths echo 'export PATH=/usr/local/anaconda3/bin:$PATH' >> ~/.bash_profile echo 'export PATH=/opt/homebrew/anaconda3/bin:$PATH' >> ~/.bash_profile source ~/.bash_profile","title":"Pre-requisites"},{"location":"cluster_usage/fasrc_basic_cluster_usage/#jargon","text":"Note that these are not the exact technical definitions of these terms, but my (potentially simplistic) understanding of them. FASRC = FAS Research Computing Cannon = Cluster for level 2 data. This is where most of the academic group's data lives. FASSE = FAS Secure Environment. Cluster for level 3 data. This is where the applied projects' data will live. SLURM = a job scheduler that is used to manage jobs on the cluster. This is the scheduler that FASRC uses. Partition = A portion of the cluster that is reserved for a specific purpose. For example, the gpu partition is reserved for GPU jobs. Job = A task that is submitted to the cluster. A job can be a single task, or a series of tasks that are run in sequence. Node = an individual server. Core = a single CPU.","title":"Jargon"},{"location":"cluster_usage/fasrc_basic_cluster_usage/#storage","text":"FASRC gives every user 100GB of storage space in your home folder. In addition, the Growth Lab has lab storage available at the following locations: - /n/hausmann_lab/lab/ - /n/holylfs05/LABS/hausmann_lab Applied projects will be moved to FASSE. The GL data committee has submitted requests to FASRC to move RCE projects to FASSE, and the requests are being considered on an individual basis by FASRC for data security review.","title":"Storage"},{"location":"cluster_usage/fasrc_basic_cluster_usage/#working-with-vdi","text":"Great guides available directly on FASRC documentation page: Using VDI VDI apps FASSE VDI apps","title":"Working with VDI"},{"location":"cluster_usage/fasrc_remote_python/","text":"Remote Python Usage \u00b6 Note that Python can be run either through VDI or through the terminal. Running Python through VDI \u00b6 Running Python through the terminal \u00b6 This is an example workflow for running Python through the terminal. The steps to follow are: Connect to the cluster Run the batch submission script Run cat juypter.* and copy the URL Open VSCode and use remote-ssh to SSH into the cluster Connect to the remote Jupyter server by pasting the URL (bottom right of your VSCode window) Connect to the appropriate conda environment (top right of your window) JupyterLab - example batch submission script #!/bin/bash #SBATCH --partition=test #SBATCH --job-name=jupyter #SBATCH --ntasks=1 #SBATCH --cpus-per-task=5 #SBATCH --mem-per-cpu=36GB #SBATCH -t 0-08:00 #SBATCH -e jupyter.err #SBATCH -o jupyter.out #SBATCH --mail-type=BEGIN,END,FAIL # get tunneling info XDG_RUNTIME_DIR = \"\" # port=$(shuf -i8000-9999 -n1) port = 9258 node = $( hostname -s ) user = $( whoami ) cluster = $( hostname -f ) echo $cluster # print tunneling instructions jupyter-log echo -e \" MacOS or linux terminal command to create your ssh tunnel ssh -NL ${ port } : ${ cluster } : ${ port } -NL 8788: ${ cluster } :8788 ${ user } @login.rc.fas.harvard.edu Forwarded port:same as remote port Remote server: ${ node } Remote port: ${ port } SSH server: ${ cluster } .rc.fas.harvard.edu SSH login: $user SSH port: 22 Use a Browser on your local machine to go to: localhost: ${ port } (prefix w/ https:// if using password) \" module load Anaconda3/2019.10 source activate cid echo $CONDA_DEFAULT_ENV jupyter lab --no-browser --port = ${ port } --ip = '0.0.0.0'","title":"Running Python Remotely"},{"location":"cluster_usage/fasrc_remote_python/#remote-python-usage","text":"Note that Python can be run either through VDI or through the terminal.","title":"Remote Python Usage"},{"location":"cluster_usage/fasrc_remote_python/#running-python-through-vdi","text":"","title":"Running Python through VDI"},{"location":"cluster_usage/fasrc_remote_python/#running-python-through-the-terminal","text":"This is an example workflow for running Python through the terminal. The steps to follow are: Connect to the cluster Run the batch submission script Run cat juypter.* and copy the URL Open VSCode and use remote-ssh to SSH into the cluster Connect to the remote Jupyter server by pasting the URL (bottom right of your VSCode window) Connect to the appropriate conda environment (top right of your window) JupyterLab - example batch submission script #!/bin/bash #SBATCH --partition=test #SBATCH --job-name=jupyter #SBATCH --ntasks=1 #SBATCH --cpus-per-task=5 #SBATCH --mem-per-cpu=36GB #SBATCH -t 0-08:00 #SBATCH -e jupyter.err #SBATCH -o jupyter.out #SBATCH --mail-type=BEGIN,END,FAIL # get tunneling info XDG_RUNTIME_DIR = \"\" # port=$(shuf -i8000-9999 -n1) port = 9258 node = $( hostname -s ) user = $( whoami ) cluster = $( hostname -f ) echo $cluster # print tunneling instructions jupyter-log echo -e \" MacOS or linux terminal command to create your ssh tunnel ssh -NL ${ port } : ${ cluster } : ${ port } -NL 8788: ${ cluster } :8788 ${ user } @login.rc.fas.harvard.edu Forwarded port:same as remote port Remote server: ${ node } Remote port: ${ port } SSH server: ${ cluster } .rc.fas.harvard.edu SSH login: $user SSH port: 22 Use a Browser on your local machine to go to: localhost: ${ port } (prefix w/ https:// if using password) \" module load Anaconda3/2019.10 source activate cid echo $CONDA_DEFAULT_ENV jupyter lab --no-browser --port = ${ port } --ip = '0.0.0.0'","title":"Running Python through the terminal"},{"location":"cluster_usage/intro_page_2021/","text":"Effective Cluster Usage and Data Management \u00b6 Tuesday, Sep 07, 2021. Pre-requisites \u00b6 Please perform the following steps before the workshop , in the interest of time: You should have an account on the RCE Cluster . If you do not have an account, look at Accessing the RCE for instructions on how to obtain one. You should be able to login to the cluster using the graphical client instructions here Setup \u00b6 If you're using Windows, download and install PuTTY If you're on a Mac, download and install Xquartz Download and install FileZilla If you can follow the instructions here , go ahead and configure FileZilla. Download and install Atom If you already know how to, go ahead and install the following packages for Atom: remote-ftp hydrogen language-stata Download and install miniconda , if you don't already have conda. Workshop Topics \u00b6 Project folder structures and data management How to achieve sanity when your project's folders and data are a mess (i.e. restructuring your project's folders) Data management for CID Planning for a new project Folder structures Best practices and documentation Scripts for reorganizing existing folders Cluster usage Basic terminal usage How to submit jobs on the cluster and manage them Using Atom to edit files remotely Using FileZilla to upload / download files Using Python / Stata remotely Tips / tricks: How to write code that other people can run How to run Python / R / Stata code on the cluster without explicitly having to open the cluster's interface Brief intro to parallel computing","title":"Effective Cluster Usage and Data Management"},{"location":"cluster_usage/intro_page_2021/#effective-cluster-usage-and-data-management","text":"Tuesday, Sep 07, 2021.","title":"Effective Cluster Usage and Data Management"},{"location":"cluster_usage/intro_page_2021/#pre-requisites","text":"Please perform the following steps before the workshop , in the interest of time: You should have an account on the RCE Cluster . If you do not have an account, look at Accessing the RCE for instructions on how to obtain one. You should be able to login to the cluster using the graphical client instructions here","title":"Pre-requisites"},{"location":"cluster_usage/intro_page_2021/#setup","text":"If you're using Windows, download and install PuTTY If you're on a Mac, download and install Xquartz Download and install FileZilla If you can follow the instructions here , go ahead and configure FileZilla. Download and install Atom If you already know how to, go ahead and install the following packages for Atom: remote-ftp hydrogen language-stata Download and install miniconda , if you don't already have conda.","title":"Setup"},{"location":"cluster_usage/intro_page_2021/#workshop-topics","text":"Project folder structures and data management How to achieve sanity when your project's folders and data are a mess (i.e. restructuring your project's folders) Data management for CID Planning for a new project Folder structures Best practices and documentation Scripts for reorganizing existing folders Cluster usage Basic terminal usage How to submit jobs on the cluster and manage them Using Atom to edit files remotely Using FileZilla to upload / download files Using Python / Stata remotely Tips / tricks: How to write code that other people can run How to run Python / R / Stata code on the cluster without explicitly having to open the cluster's interface Brief intro to parallel computing","title":"Workshop Topics"},{"location":"cluster_usage/rce_basic_cluster_usage/","text":"Basic cluster usage \u00b6 Working with Linux \u00b6 The RCE cluster runs on Linux. This section discusses some commands that might be useful for everday cluster usage: Change directory: cd <path to directory> Create new folder in current directory: mkdir <name of new folder> List contents of current directory: ls Including hidden files: ls -al With file sizes in KB/MB/GB instead of bytes: ls -alh Create an empty file: touch <filename> Open a Vim text editor: vi <filename> (There are a bunch of vim specific commands you need to learn to use Vim) Output the contents of a text file: cat <filename> Output the first few lines of a file: head <filename> Copy file: cp <old_file_path> <new_file_path> Move file: mv <old_file_path> <new_file_path> Rename file: mv <old_file_name> <new_file_name> Remove file: rm <file_name> For more commonly used commands, here's a cheatsheet RCE-Specific Commands \u00b6 The RCE cluster uses a framework called HTCondor. The following commands can be used for any cluster that runs on HTCondor. Connect to the RCE login node: ssh <username>@rce.hmdc.harvard.edu Connect to the RCE login node with port-forwarding: ssh -L 8889:localhost:8889 <username>@rce.hmdc.harvard.edu Check the status of your jobs: condor_q -global <username> SSH to job from the login node: condor_ssh_to_job -name \"<name of machine where job is running>\" <JobID> Check available resources: rce-info.sh Submit jobs: condor_submit <submit_file_path> or condor_submit_util Remove running jobs: condor_rm -name \"<name of the machine where job is running>\" <JobID> Check disk usage: quotareport Optional Tips \u00b6 Other Relevant Commands \u00b6 Start new tmux session: tmux new Re-attach the last tmux session: tmux a Kill all running tmux sessions: tmux kill-server Setting up SSH Key Access \u00b6 If you don't want to type your password each time you SSH (from a computer you trust, of course), set up SSH keys. Steps (for Linux and MacOS): Check for existing SSH keys If you don't have existing keys, generate a key If you have existing keys, add to ssh-agent Upload the key to the remote server, using ssh-copy-id -i ~/.ssh/id_rsa.pub <username>@rce.hmdc.harvard.edu","title":"Basic Commands"},{"location":"cluster_usage/rce_basic_cluster_usage/#basic-cluster-usage","text":"","title":"Basic cluster usage"},{"location":"cluster_usage/rce_basic_cluster_usage/#working-with-linux","text":"The RCE cluster runs on Linux. This section discusses some commands that might be useful for everday cluster usage: Change directory: cd <path to directory> Create new folder in current directory: mkdir <name of new folder> List contents of current directory: ls Including hidden files: ls -al With file sizes in KB/MB/GB instead of bytes: ls -alh Create an empty file: touch <filename> Open a Vim text editor: vi <filename> (There are a bunch of vim specific commands you need to learn to use Vim) Output the contents of a text file: cat <filename> Output the first few lines of a file: head <filename> Copy file: cp <old_file_path> <new_file_path> Move file: mv <old_file_path> <new_file_path> Rename file: mv <old_file_name> <new_file_name> Remove file: rm <file_name> For more commonly used commands, here's a cheatsheet","title":"Working with Linux"},{"location":"cluster_usage/rce_basic_cluster_usage/#rce-specific-commands","text":"The RCE cluster uses a framework called HTCondor. The following commands can be used for any cluster that runs on HTCondor. Connect to the RCE login node: ssh <username>@rce.hmdc.harvard.edu Connect to the RCE login node with port-forwarding: ssh -L 8889:localhost:8889 <username>@rce.hmdc.harvard.edu Check the status of your jobs: condor_q -global <username> SSH to job from the login node: condor_ssh_to_job -name \"<name of machine where job is running>\" <JobID> Check available resources: rce-info.sh Submit jobs: condor_submit <submit_file_path> or condor_submit_util Remove running jobs: condor_rm -name \"<name of the machine where job is running>\" <JobID> Check disk usage: quotareport","title":"RCE-Specific Commands"},{"location":"cluster_usage/rce_basic_cluster_usage/#optional-tips","text":"","title":"Optional Tips"},{"location":"cluster_usage/rce_basic_cluster_usage/#other-relevant-commands","text":"Start new tmux session: tmux new Re-attach the last tmux session: tmux a Kill all running tmux sessions: tmux kill-server","title":"Other Relevant Commands"},{"location":"cluster_usage/rce_basic_cluster_usage/#setting-up-ssh-key-access","text":"If you don't want to type your password each time you SSH (from a computer you trust, of course), set up SSH keys. Steps (for Linux and MacOS): Check for existing SSH keys If you don't have existing keys, generate a key If you have existing keys, add to ssh-agent Upload the key to the remote server, using ssh-copy-id -i ~/.ssh/id_rsa.pub <username>@rce.hmdc.harvard.edu","title":"Setting up SSH Key Access"},{"location":"cluster_usage/rce_remote_python_options/","text":"Option 1: Jupyter Lab \u00b6 Note Compared to working with Jupyter notebooks on the cluster using the NoMachine GUI (which is stuttery because of high latency), running them remotely provides a convenient low latency alternative Although Jupyter notebooks have become the de facto method to work on Python, since they are not simple text files, they are harder to manage using Git than .py scripts Setup \u00b6 SSH into the cluster, using portforwarding # Replace \"<rce_username>\" with your RCE username ssh -L 8889 :localhost:8889 <rce_username>@rce.hmdc.harvard.edu Load conda into the shell and set necessary environment variables Warning Be careful while editing ~/.bashrc and ~/.bash_profile files - they are executed each time your shell opens, and you don't want to mess it up and be unable to access your account. Add the following lines to your ~/.bashrc (replace <path_to_environment> and <path_to_packages> ): # Start conda . /nfs/tools/lib/anaconda/3-5.2.0/etc/profile.d/conda.sh # Tell conda where to save and look for environments # Example: export CONDA_ENVS_PATH=\"/nfs/projects_nobackup_ci3/m/ci3_mastercard/shreyas/utils/envs/\" export CONDA_ENVS_PATH = \"<path_to_environment>\" # Tell conda where to save and look for packages # Example: export CONDA_PKGS_DIRS=\"/nfs/projects_nobackup_ci3/m/ci3_mastercard/shreyas/utils/pkgs/\" export CONDA_PKGS_DIRS = \"<path_to_packages>\" If you aren't sure how to edit your .bashrc , run the following commands and it'll do it for you (you still have to replace <path_to_environment> and <path_to_packages> ): echo \". /nfs/tools/lib/anaconda/3-5.2.0/etc/profile.d/conda.sh\" >> ~/.bashrc # Set required environment variables to specify location of environment echo \"export CONDA_ENVS_PATH=\\\"<path_to_environment>\\\"\" >> ~/.bashrc echo \"export CONDA_PKGS_DIRS=\\\"<path_to_packages>\\\"\" >> ~/.bashrc Rerun .bashrc : . ~/.bashrc Create and prepare a conda environment, and activate it with the following commands: # Navigate to <path_to_environment> cd <path_to_environment> # Create conda environment in the current folder conda create --prefix = cid python = 3 # Activate conda environment (can now be done from any folder) conda activate cid # Install necessary packages ## Add conda-forge as the main channel for downloading packages (optional) conda config --add channels conda-forge ## Download required packages conda install -c conda-forge jupyterlab nodejs Prepare condor submission and connection scripts (replace <your_token> ) # Make a directory somewhere to house the condor scripts mkdir -p ~/condorscripts/condorlogs && cd ~/condorscripts # Download Jupyter submission script from Github Repo curl -O https://raw.githubusercontent.com/cid-harvard/workshop-cluster-training/master/assets/condorscripts/jupyter.submit # Automatically replace \"~\" in jupyter.submit with the absolute path to your HOME directory sed -i 's@\\~@' \" $HOME \" '@' jupyter.submit # Download Jupyter connection script curl -O https://raw.githubusercontent.com/cid-harvard/workshop-cluster-training/master/assets/condorscripts/condorsshrce.sh # Automatically replace \"username\" with the username sed -i 's/username/' \" $USER \" '/' condorsshrce.sh # Download shell script to activate conda env named 'cid' and run Jupyter curl -O https://raw.githubusercontent.com/cid-harvard/workshop-cluster-training/master/assets/condorscripts/run_jupyter.sh # Replace \"<your_token>\" with token of your choice # Example: sed -i 's/my_token/example_token/' run_jupyter.sh sed -i 's/my_token/<your_token>/' run_jupyter.sh Running Python through Jupyter \u00b6 SSH into the cluster, using portforwarding (if you haven't yet) # Replace \"<rce_username>\" with your RCE username ssh -L 8889 :localhost:8889 <rce_username>@rce.hmdc.harvard.edu Submit Jupyter job # Submit condor script condor_submit ~/condorscripts/jupyter.submit If job submission system is buggy (it usually is), then follow these steps: Open NoMachine GUI Go to Applications -> RCE Powered Applications -> Anaconda Shell (Python 3) Connect to the Jupyter server # SSH to the machine running your jupyter server . ~/condorscripts/condorsshrce.sh $USER Tip: tmux keyboard shortcuts and commands New tmux window: tmux new Detach tmux window: ctrl+b , then d Reattach latest tmux window: tmux a New horizontal pane: ctrl+b , then \" New vertical pane: ctrl+b , then % Kill current pane: ctrl+b , then x , then y Move between panes: ctrl+b , then arrow keys Stop moving, or cancel a tmux-specific command: escape Kill all tmux sessions: tmux kill-server In your browser, go to localhost:8889 , and voila! Once you're done, you can close the compute node using ctrl+d , you might then have to press ctrl+c if your login node is taking time to appear Remember to remove the job once you're finished: # Look up running jobs condor_q -global $USER # Remove job condor_rm -name \"<machine_name>\" <ID> Example Option 2: Atom + Hydrogen \u00b6 Note The text editor Atom, using the package Hydrogen, allows you to run code interactively, inspect data and plot using Jupyter kernels. Allows the option of working on .py files directly, with low latency. Setup \u00b6 Setting up the Jupyter Kernel \u00b6 The steps for setting up the Jupyter kernel are the same as for Option 1: Jupyter Lab . Setting up Atom \u00b6 You can learn some Atom basics here Install required packages To install a package on atom, press Cmd+Shift+P (Mac) or Cmd+Shift+P (Windows) to enter the \"Command Palette\", and type Install Packages . Install the following packages: remote-ftp : enable browsing remote files hydrogen : run code through jupyter kernels Optional (my favourite add-ons): tree-view : explore files in project file-icons : convenient file icons in tree view atom-beautify : automatically indent / beautify code according to linters open-recent : open recently opened files / projects teletype : collaborate on code in real-time (think google docs for code) highlight-selected : highlight all occurrences of selected word or phrase minimap : mini view of the code on the side Configure remote_ftp Create an empty folder in your computer. Call it remote_atom (or any other name you might see fit). Open Atom and click \"File -> Open -> remote_atom \". This will open the remote_atom folder as a \"project\". Open the Command Palette ( Cmd+Shift+P or Ctrl+Shift+P ) and type \"Create Sftp\" and choose the option \"Remote Ftp: Create Sftp Config File\". You will notice that a file named .ftpconfig is automatically created in the same folder. Replace the contents of the file with the following, modifying the user and remote parameters: { \"protocol\" : \"sftp\" , \"host\" : \"rce.hmdc.harvard.edu\" , // string - Hostname or IP address of the server. Default: 'localhost' \"port\" : 22 , // integer - Port number of the server. Default: 22 \"user\" : \"<username>\" , // string - Username for authentication. Default: (none) \"promptForPass\" : true , // boolean - Set to true for enable password/passphrase dialog. This will prevent from using cleartext password/passphrase in this config. Default: false \"remote\" : \"/nfs/home/S/shg309/ <replace with your own home folder>\" , // try to use absolute paths starting with / \"connTimeout\" : 10000 , // integer - How long (in milliseconds) to wait for the SSH handshake to complete. Default: 10000 \"keepalive\" : 10000 // integer - How often (in milliseconds) to send SSH-level keepalive packets to the server (in a similar way as OpenSSH's ServerAliveInterval config option). Set to 0 to disable. Default: 10000 } Open the Command Palette and type \"Remote Ftp\", and choose the option Toggle . This will open up a Remote tab on the left hand side. In the Remote tab, click on Connect , and voila! You can now edit files on the cluster as if they were on your own computer! Warning Don't use the remote_atom project unless you're working on the cluster. The files in the folder are uploaded automatically to the cluster when you reconnect. Note Once you're done, remember to Disconnect . Optional: Through the Command Palette, go to \"View Installed Packages\" --> remote_ftp's settings --> change Auto Upload on Save from always to only when connected Configure Hydrogen Through the Command Palette, go to \"View Installed Packages\" --> hydrogen's settings --> change Kernel Gateways to the following (replace <your_token> with the token you set for Jupyter earlier): [{ \"name\" : \"RCE Jupyter\" , \"options\" :{ \"baseUrl\" : \"http://localhost:8889\" , \"token\" : \"<your_token>\" }}] Running Python through Hydrogen \u00b6 Connect to Remote Kernel Open a Python .py script Through the Command Palette, go to \"Connect to Remote Kernel\". Troubleshooting If you haven't set a token, you might be asked to \"Authenticate Using Token\". To find the authentication token, on an RCE node, type the following: # Activate conda environment conda activate cid # Get Jupyter token jupyter notebook list The token will be of the form: http://localhost:8889/?token= a61elkjziunns1523e70eb18a452cfsdf9812302199b87e Running code Select code you want to run, and hit Cmd+Enter (Mac) or Ctrl+Enter (Windows) To run all code, hit Cmd+Option+Enter (Mac) or Ctrl+Alt+Enter (Windows) Magics You can use Jupyter magics through Hydrogen % reset - f # Reset all variables in memory % matplotlib inline # Inline matplotlib figures import numpy as np import pandas as pd import matplotlib.pyplot as plt # Simple tests print ( 'Hello world!' ) 17 + 25 # Print dataframe pd . DataFrame ({ 'col1' : [ 1 , 2 ], 'col2' : [ 3 , 4 ]}) # Show plot N = 50 x = np . random . rand ( N ) y = np . random . rand ( N ) colors = np . random . rand ( N ) area = ( 30 * np . random . rand ( N )) ** 2 # 0 to 15 point radii plt . scatter ( x , y , s = area , c = colors , alpha = 0.5 ) plt . show ()","title":"Running Python Remotely"},{"location":"cluster_usage/rce_remote_python_options/#option-1-jupyter-lab","text":"Note Compared to working with Jupyter notebooks on the cluster using the NoMachine GUI (which is stuttery because of high latency), running them remotely provides a convenient low latency alternative Although Jupyter notebooks have become the de facto method to work on Python, since they are not simple text files, they are harder to manage using Git than .py scripts","title":"Option 1: Jupyter Lab"},{"location":"cluster_usage/rce_remote_python_options/#setup","text":"SSH into the cluster, using portforwarding # Replace \"<rce_username>\" with your RCE username ssh -L 8889 :localhost:8889 <rce_username>@rce.hmdc.harvard.edu Load conda into the shell and set necessary environment variables Warning Be careful while editing ~/.bashrc and ~/.bash_profile files - they are executed each time your shell opens, and you don't want to mess it up and be unable to access your account. Add the following lines to your ~/.bashrc (replace <path_to_environment> and <path_to_packages> ): # Start conda . /nfs/tools/lib/anaconda/3-5.2.0/etc/profile.d/conda.sh # Tell conda where to save and look for environments # Example: export CONDA_ENVS_PATH=\"/nfs/projects_nobackup_ci3/m/ci3_mastercard/shreyas/utils/envs/\" export CONDA_ENVS_PATH = \"<path_to_environment>\" # Tell conda where to save and look for packages # Example: export CONDA_PKGS_DIRS=\"/nfs/projects_nobackup_ci3/m/ci3_mastercard/shreyas/utils/pkgs/\" export CONDA_PKGS_DIRS = \"<path_to_packages>\" If you aren't sure how to edit your .bashrc , run the following commands and it'll do it for you (you still have to replace <path_to_environment> and <path_to_packages> ): echo \". /nfs/tools/lib/anaconda/3-5.2.0/etc/profile.d/conda.sh\" >> ~/.bashrc # Set required environment variables to specify location of environment echo \"export CONDA_ENVS_PATH=\\\"<path_to_environment>\\\"\" >> ~/.bashrc echo \"export CONDA_PKGS_DIRS=\\\"<path_to_packages>\\\"\" >> ~/.bashrc Rerun .bashrc : . ~/.bashrc Create and prepare a conda environment, and activate it with the following commands: # Navigate to <path_to_environment> cd <path_to_environment> # Create conda environment in the current folder conda create --prefix = cid python = 3 # Activate conda environment (can now be done from any folder) conda activate cid # Install necessary packages ## Add conda-forge as the main channel for downloading packages (optional) conda config --add channels conda-forge ## Download required packages conda install -c conda-forge jupyterlab nodejs Prepare condor submission and connection scripts (replace <your_token> ) # Make a directory somewhere to house the condor scripts mkdir -p ~/condorscripts/condorlogs && cd ~/condorscripts # Download Jupyter submission script from Github Repo curl -O https://raw.githubusercontent.com/cid-harvard/workshop-cluster-training/master/assets/condorscripts/jupyter.submit # Automatically replace \"~\" in jupyter.submit with the absolute path to your HOME directory sed -i 's@\\~@' \" $HOME \" '@' jupyter.submit # Download Jupyter connection script curl -O https://raw.githubusercontent.com/cid-harvard/workshop-cluster-training/master/assets/condorscripts/condorsshrce.sh # Automatically replace \"username\" with the username sed -i 's/username/' \" $USER \" '/' condorsshrce.sh # Download shell script to activate conda env named 'cid' and run Jupyter curl -O https://raw.githubusercontent.com/cid-harvard/workshop-cluster-training/master/assets/condorscripts/run_jupyter.sh # Replace \"<your_token>\" with token of your choice # Example: sed -i 's/my_token/example_token/' run_jupyter.sh sed -i 's/my_token/<your_token>/' run_jupyter.sh","title":"Setup"},{"location":"cluster_usage/rce_remote_python_options/#running-python-through-jupyter","text":"SSH into the cluster, using portforwarding (if you haven't yet) # Replace \"<rce_username>\" with your RCE username ssh -L 8889 :localhost:8889 <rce_username>@rce.hmdc.harvard.edu Submit Jupyter job # Submit condor script condor_submit ~/condorscripts/jupyter.submit If job submission system is buggy (it usually is), then follow these steps: Open NoMachine GUI Go to Applications -> RCE Powered Applications -> Anaconda Shell (Python 3) Connect to the Jupyter server # SSH to the machine running your jupyter server . ~/condorscripts/condorsshrce.sh $USER Tip: tmux keyboard shortcuts and commands New tmux window: tmux new Detach tmux window: ctrl+b , then d Reattach latest tmux window: tmux a New horizontal pane: ctrl+b , then \" New vertical pane: ctrl+b , then % Kill current pane: ctrl+b , then x , then y Move between panes: ctrl+b , then arrow keys Stop moving, or cancel a tmux-specific command: escape Kill all tmux sessions: tmux kill-server In your browser, go to localhost:8889 , and voila! Once you're done, you can close the compute node using ctrl+d , you might then have to press ctrl+c if your login node is taking time to appear Remember to remove the job once you're finished: # Look up running jobs condor_q -global $USER # Remove job condor_rm -name \"<machine_name>\" <ID> Example","title":"Running Python through Jupyter"},{"location":"cluster_usage/rce_remote_python_options/#option-2-atom-hydrogen","text":"Note The text editor Atom, using the package Hydrogen, allows you to run code interactively, inspect data and plot using Jupyter kernels. Allows the option of working on .py files directly, with low latency.","title":"Option 2: Atom + Hydrogen"},{"location":"cluster_usage/rce_remote_python_options/#setup_1","text":"","title":"Setup"},{"location":"cluster_usage/rce_remote_python_options/#setting-up-the-jupyter-kernel","text":"The steps for setting up the Jupyter kernel are the same as for Option 1: Jupyter Lab .","title":"Setting up the Jupyter Kernel"},{"location":"cluster_usage/rce_remote_python_options/#setting-up-atom","text":"You can learn some Atom basics here Install required packages To install a package on atom, press Cmd+Shift+P (Mac) or Cmd+Shift+P (Windows) to enter the \"Command Palette\", and type Install Packages . Install the following packages: remote-ftp : enable browsing remote files hydrogen : run code through jupyter kernels Optional (my favourite add-ons): tree-view : explore files in project file-icons : convenient file icons in tree view atom-beautify : automatically indent / beautify code according to linters open-recent : open recently opened files / projects teletype : collaborate on code in real-time (think google docs for code) highlight-selected : highlight all occurrences of selected word or phrase minimap : mini view of the code on the side Configure remote_ftp Create an empty folder in your computer. Call it remote_atom (or any other name you might see fit). Open Atom and click \"File -> Open -> remote_atom \". This will open the remote_atom folder as a \"project\". Open the Command Palette ( Cmd+Shift+P or Ctrl+Shift+P ) and type \"Create Sftp\" and choose the option \"Remote Ftp: Create Sftp Config File\". You will notice that a file named .ftpconfig is automatically created in the same folder. Replace the contents of the file with the following, modifying the user and remote parameters: { \"protocol\" : \"sftp\" , \"host\" : \"rce.hmdc.harvard.edu\" , // string - Hostname or IP address of the server. Default: 'localhost' \"port\" : 22 , // integer - Port number of the server. Default: 22 \"user\" : \"<username>\" , // string - Username for authentication. Default: (none) \"promptForPass\" : true , // boolean - Set to true for enable password/passphrase dialog. This will prevent from using cleartext password/passphrase in this config. Default: false \"remote\" : \"/nfs/home/S/shg309/ <replace with your own home folder>\" , // try to use absolute paths starting with / \"connTimeout\" : 10000 , // integer - How long (in milliseconds) to wait for the SSH handshake to complete. Default: 10000 \"keepalive\" : 10000 // integer - How often (in milliseconds) to send SSH-level keepalive packets to the server (in a similar way as OpenSSH's ServerAliveInterval config option). Set to 0 to disable. Default: 10000 } Open the Command Palette and type \"Remote Ftp\", and choose the option Toggle . This will open up a Remote tab on the left hand side. In the Remote tab, click on Connect , and voila! You can now edit files on the cluster as if they were on your own computer! Warning Don't use the remote_atom project unless you're working on the cluster. The files in the folder are uploaded automatically to the cluster when you reconnect. Note Once you're done, remember to Disconnect . Optional: Through the Command Palette, go to \"View Installed Packages\" --> remote_ftp's settings --> change Auto Upload on Save from always to only when connected Configure Hydrogen Through the Command Palette, go to \"View Installed Packages\" --> hydrogen's settings --> change Kernel Gateways to the following (replace <your_token> with the token you set for Jupyter earlier): [{ \"name\" : \"RCE Jupyter\" , \"options\" :{ \"baseUrl\" : \"http://localhost:8889\" , \"token\" : \"<your_token>\" }}]","title":"Setting up Atom"},{"location":"cluster_usage/rce_remote_python_options/#running-python-through-hydrogen","text":"Connect to Remote Kernel Open a Python .py script Through the Command Palette, go to \"Connect to Remote Kernel\". Troubleshooting If you haven't set a token, you might be asked to \"Authenticate Using Token\". To find the authentication token, on an RCE node, type the following: # Activate conda environment conda activate cid # Get Jupyter token jupyter notebook list The token will be of the form: http://localhost:8889/?token= a61elkjziunns1523e70eb18a452cfsdf9812302199b87e Running code Select code you want to run, and hit Cmd+Enter (Mac) or Ctrl+Enter (Windows) To run all code, hit Cmd+Option+Enter (Mac) or Ctrl+Alt+Enter (Windows) Magics You can use Jupyter magics through Hydrogen % reset - f # Reset all variables in memory % matplotlib inline # Inline matplotlib figures import numpy as np import pandas as pd import matplotlib.pyplot as plt # Simple tests print ( 'Hello world!' ) 17 + 25 # Print dataframe pd . DataFrame ({ 'col1' : [ 1 , 2 ], 'col2' : [ 3 , 4 ]}) # Show plot N = 50 x = np . random . rand ( N ) y = np . random . rand ( N ) colors = np . random . rand ( N ) area = ( 30 * np . random . rand ( N )) ** 2 # 0 to 15 point radii plt . scatter ( x , y , s = area , c = colors , alpha = 0.5 ) plt . show ()","title":"Running Python through Hydrogen"},{"location":"cluster_usage/rce_remote_r_options/","text":"Running R on Jupyter \u00b6 Use IRKernel: https://irkernel.github.io/installation/#binary-panel To install: Use conda conda install -c r r-irkernel Make kernel available to jupyter At terminal: R At the R-prompt: IRkernel :: installspec ()","title":"Running R on Jupyter"},{"location":"cluster_usage/rce_remote_r_options/#running-r-on-jupyter","text":"Use IRKernel: https://irkernel.github.io/installation/#binary-panel To install: Use conda conda install -c r r-irkernel Make kernel available to jupyter At terminal: R At the R-prompt: IRkernel :: installspec ()","title":"Running R on Jupyter"},{"location":"cluster_usage/rce_remote_stata_options/","text":"Option 1: X-forwarding \u00b6 Note Gives you the actual window running Stata, but suffers from latency issues, which means that the screen will be stuttery Setup \u00b6 MacOS: Install Xquartz Edit ~/.ssh/config and add: Host * XAuthLocation /opt/X11/bin/xauth ForwardX11 yes ForwardX11Trusted yes Windows: In PuTTy, enable X11 forwarding Linux: No setup required! X11 is pre-installed You might have to edit ~/.ssh/config to allow X11 forwarding (same as MacOS) Running Stata \u00b6 ssh to the RCE, adding a -Y flag to the command. You can use -X (untrusted X11 forwarding) or -Y (trusted X11 forwarding, slightly smoother) -Y is less secure, so only use it for applications you recognize (such as Stata) # Replace \"<rce_username>\" with your RCE username ssh -Y <rce_username>@rce.hmdc.harvard.edu Run the RCE provided convenience-command to start STATA jobs, with a graphical interface: rce_submit.py -r -graphical -a xstata-mp For commonly used commands and introductory tutorials, refer to RCE documentation . Option 2: Jupyter Lab \u00b6 Note Thanks to Kyle Barron's package stata_kernel , we can use Stata kernels for Jupyter, allowing us to run Stata remotely with low latency. However, Jupyter notebooks are not text files, so working with them does not have the do-file editor feel that Stata users might be used to. Additionally, Jupyter notebooks are harder to manage using Git Setup \u00b6 SSH into the cluster, using portforwarding # Replace \"<rce_username>\" with your RCE username ssh -L 8889 :localhost:8889 <rce_username>@rce.hmdc.harvard.edu Load conda into the shell and set necessary environment variables Warning Be careful while editing ~/.bashrc and ~/.bash_profile files - they are executed each time your shell opens, and you don't want to mess it up and be unable to access your account. Add the following lines to your ~/.bashrc (replace <path_to_environment> and <path_to_packages> ): # Start conda . /nfs/tools/lib/anaconda/3-5.2.0/etc/profile.d/conda.sh # Tell conda where to save and look for environments # Example: export CONDA_ENVS_PATH=\"/nfs/projects_nobackup_ci3/m/ci3_mastercard/shreyas/utils/envs/\" export CONDA_ENVS_PATH = \"<path_to_environment>\" # Tell conda where to save and look for packages # Example: export CONDA_PKGS_DIRS=\"/nfs/projects_nobackup_ci3/m/ci3_mastercard/shreyas/utils/pkgs/\" export CONDA_PKGS_DIRS = \"<path_to_packages>\" If you aren't sure how to edit your .bashrc , run the following commands and it'll do it for you (you still have to replace <path_to_environment> and <path_to_packages> ): echo \". /nfs/tools/lib/anaconda/3-5.2.0/etc/profile.d/conda.sh\" >> ~/.bashrc # Set required environment variables to specify location of environment echo \"export CONDA_ENVS_PATH=\\\"<path_to_environment>\\\"\" >> ~/.bashrc echo \"export CONDA_PKGS_DIRS=\\\"<path_to_packages>\\\"\" >> ~/.bashrc Rerun .bashrc : . ~/.bashrc Create and prepare a conda environment, and activate it with the following commands: # Navigate to <path_to_environment> cd <path_to_environment> # Create conda environment in the current folder conda create --prefix = cid python = 3 # Activate conda environment (can now be done from any folder) conda activate cid # Install necessary packages ## Add conda-forge as the main channel for downloading packages (optional) conda config --add channels conda-forge ## Download required packages conda install -c conda-forge jupyterlab nodejs Install STATA for Jupyter ## Install stata_kernel pip install stata_kernel python -m stata_kernel.install ## Install JupyterLab Extension for Stata syntax highlighting jupyter labextension install jupyterlab-stata-highlight Prepare condor submission and connection scripts (replace <your_token> ) # Make a directory somewhere to house the condor scripts mkdir -p ~/condorscripts/condorlogs && cd ~/condorscripts # Download Jupyter submission script from Github Repo curl -O https://raw.githubusercontent.com/cid-harvard/workshop-cluster-training/master/assets/condorscripts/jupyter.submit # Automatically replace \"~\" in jupyter.submit with the absolute path to your HOME directory sed -i 's@\\~@' \" $HOME \" '@' jupyter.submit # Download Jupyter connection script curl -O https://raw.githubusercontent.com/cid-harvard/workshop-cluster-training/master/assets/condorscripts/condorsshrce.sh # Automatically replace \"username\" with the username sed -i 's/username/' \" $USER \" '/' condorsshrce.sh # Download shell script to activate conda env named 'cid' and run Jupyter curl -O https://raw.githubusercontent.com/cid-harvard/workshop-cluster-training/master/assets/condorscripts/run_jupyter.sh # Replace \"<your_token>\" with token of your choice # Example: sed -i 's/my_token/example_token/' run_jupyter.sh sed -i 's/my_token/<your_token>/' run_jupyter.sh Running Stata through Jupyter \u00b6 SSH into the cluster, using portforwarding (if you haven't yet) # Replace \"<rce_username>\" with your RCE username ssh -L 8889 :localhost:8889 <rce_username>@rce.hmdc.harvard.edu Submit Jupyter job # Submit condor script condor_submit ~/condorscripts/jupyter.submit Use tmux to handle connection errors/closures (optional) # Start a new tmux window tmux new Connect to the Jupyter server # SSH to the machine running your jupyter server . ~/condorscripts/condorsshrce.sh $USER Tip: tmux keyboard shortcuts and commands New tmux window: tmux new Detach tmux window: ctrl+b , then d Reattach latest tmux window: tmux a New horizontal pane: ctrl+b , then \" New vertical pane: ctrl+b , then % Kill current pane: ctrl+b , then x , then y Move between panes: ctrl+b , then arrow keys Stop moving, or cancel a tmux-specific command: escape Kill all tmux sessions: tmux kill-server In your browser, go to localhost:8889 , and voila! Once you're done, you can close the compute node using ctrl+d , you might then have to press ctrl+c if your login node is taking time to appear Remember to remove the job once you're finished: # Look up running jobs condor_q -global $USER # Remove job condor_rm -name \"<machine_name>\" <ID> Example Option 3: Atom + Hydrogen \u00b6 Note The text editor Atom, using the package Hydrogen, allows you to run code interactively, inspect data and plot using Jupyter kernels. This method uses stata_kernel as well. Recommended option! Provides a do-file like feel , with low latency (i.e. no stuttering). Setup \u00b6 Setting up the Jupyter Kernel \u00b6 The steps for setting up the Jupyter kernel are the same as for Option 2: Jupyter Lab . Setting up Atom \u00b6 You can learn some Atom basics here Install required packages To install a package on atom, press Cmd+Shift+P (Mac) or Cmd+Shift+P (Windows) to enter the \"Command Palette\", and type Install Packages . Install the following packages: remote-ftp : enable browsing remote files hydrogen : run code through jupyter kernels language-stata : stata code linting Optional (my favourite add-ons): tree-view : explore files in project file-icons : convenient file icons in tree view atom-beautify : automatically indent / beautify code according to linters open-recent : open recently opened files / projects teletype : collaborate on code in real-time (think google docs for code) highlight-selected : highlight all occurrences of selected word or phrase minimap : mini view of the code on the side Configure remote_ftp Create an empty folder in your computer. Call it remote_atom (or any other name you might see fit). Open Atom and click \"File -> Open -> remote_atom \". This will open the remote_atom folder as a \"project\". Open the Command Palette ( Cmd+Shift+P or Ctrl+Shift+P ) and type \"Create Sftp\" and choose the option \"Remote Ftp: Create Sftp Config File\". You will notice that a file named .ftpconfig is automatically created in the same folder. Replace the contents of the file with the following, modifying the user and remote parameters: { \"protocol\" : \"sftp\" , \"host\" : \"rce.hmdc.harvard.edu\" , // string - Hostname or IP address of the server. Default: 'localhost' \"port\" : 22 , // integer - Port number of the server. Default: 22 \"user\" : \"<username>\" , // string - Username for authentication. Default: (none) \"promptForPass\" : true , // boolean - Set to true for enable password/passphrase dialog. This will prevent from using cleartext password/passphrase in this config. Default: false \"remote\" : \"/nfs/home/S/shg309/ <replace with your own home folder>\" , // try to use absolute paths starting with / \"connTimeout\" : 10000 , // integer - How long (in milliseconds) to wait for the SSH handshake to complete. Default: 10000 \"keepalive\" : 10000 // integer - How often (in milliseconds) to send SSH-level keepalive packets to the server (in a similar way as OpenSSH's ServerAliveInterval config option). Set to 0 to disable. Default: 10000 } Open the Command Palette and type \"Remote Ftp\", and choose the option Toggle . This will open up a Remote tab on the left hand side. In the Remote tab, click on Connect , and voila! You can now edit files on the cluster as if they were on your own computer! Warning Don't use the remote_atom project unless you're working on the cluster. The files in the folder are uploaded automatically to the cluster when you reconnect. Note Once you're done, remember to Disconnect . Optional: Through the Command Palette, go to \"View Installed Packages\" --> remote_ftp's settings --> change Auto Upload on Save from always to only when connected Configure Hydrogen Through the Command Palette, go to \"View Installed Packages\" --> hydrogen's settings --> change Kernel Gateways to the following (replace <your_token> with the token you set for Jupyter earlier): [{ \"name\" : \"RCE Jupyter\" , \"options\" :{ \"baseUrl\" : \"http://localhost:8889\" , \"token\" : \"<your_token>\" }}] Running Stata through Hydrogen \u00b6 Connect to Remote Kernel Open a Stata .do file Through the Command Palette, go to \"Connect to Remote Kernel\". Troubleshooting If you haven't set a token, you might be asked to \"Authenticate Using Token\". To find the authentication token, on an RCE node, type the following: # Activate conda environment conda activate cid # Get Jupyter token jupyter notebook list The token will be of the form: http://localhost:8889/?token= a61elkjziunns1523e70eb18a452cfsdf9812302199b87e Running code Select code you want to run, and hit Cmd+Enter (Mac) or Ctrl+Enter (Windows) To run all code, hit Cmd+Option+Enter (Mac) or Ctrl+Alt+Enter (Windows) Magics For browsing data, you can use the %browse magic: webuse auto scatter mpg weight %browse 100 mpg Refer to this page for more Jupyter+Stata magics.","title":"Running Stata Remotely"},{"location":"cluster_usage/rce_remote_stata_options/#option-1-x-forwarding","text":"Note Gives you the actual window running Stata, but suffers from latency issues, which means that the screen will be stuttery","title":"Option 1: X-forwarding"},{"location":"cluster_usage/rce_remote_stata_options/#setup","text":"MacOS: Install Xquartz Edit ~/.ssh/config and add: Host * XAuthLocation /opt/X11/bin/xauth ForwardX11 yes ForwardX11Trusted yes Windows: In PuTTy, enable X11 forwarding Linux: No setup required! X11 is pre-installed You might have to edit ~/.ssh/config to allow X11 forwarding (same as MacOS)","title":"Setup"},{"location":"cluster_usage/rce_remote_stata_options/#running-stata","text":"ssh to the RCE, adding a -Y flag to the command. You can use -X (untrusted X11 forwarding) or -Y (trusted X11 forwarding, slightly smoother) -Y is less secure, so only use it for applications you recognize (such as Stata) # Replace \"<rce_username>\" with your RCE username ssh -Y <rce_username>@rce.hmdc.harvard.edu Run the RCE provided convenience-command to start STATA jobs, with a graphical interface: rce_submit.py -r -graphical -a xstata-mp For commonly used commands and introductory tutorials, refer to RCE documentation .","title":"Running Stata"},{"location":"cluster_usage/rce_remote_stata_options/#option-2-jupyter-lab","text":"Note Thanks to Kyle Barron's package stata_kernel , we can use Stata kernels for Jupyter, allowing us to run Stata remotely with low latency. However, Jupyter notebooks are not text files, so working with them does not have the do-file editor feel that Stata users might be used to. Additionally, Jupyter notebooks are harder to manage using Git","title":"Option 2: Jupyter Lab"},{"location":"cluster_usage/rce_remote_stata_options/#setup_1","text":"SSH into the cluster, using portforwarding # Replace \"<rce_username>\" with your RCE username ssh -L 8889 :localhost:8889 <rce_username>@rce.hmdc.harvard.edu Load conda into the shell and set necessary environment variables Warning Be careful while editing ~/.bashrc and ~/.bash_profile files - they are executed each time your shell opens, and you don't want to mess it up and be unable to access your account. Add the following lines to your ~/.bashrc (replace <path_to_environment> and <path_to_packages> ): # Start conda . /nfs/tools/lib/anaconda/3-5.2.0/etc/profile.d/conda.sh # Tell conda where to save and look for environments # Example: export CONDA_ENVS_PATH=\"/nfs/projects_nobackup_ci3/m/ci3_mastercard/shreyas/utils/envs/\" export CONDA_ENVS_PATH = \"<path_to_environment>\" # Tell conda where to save and look for packages # Example: export CONDA_PKGS_DIRS=\"/nfs/projects_nobackup_ci3/m/ci3_mastercard/shreyas/utils/pkgs/\" export CONDA_PKGS_DIRS = \"<path_to_packages>\" If you aren't sure how to edit your .bashrc , run the following commands and it'll do it for you (you still have to replace <path_to_environment> and <path_to_packages> ): echo \". /nfs/tools/lib/anaconda/3-5.2.0/etc/profile.d/conda.sh\" >> ~/.bashrc # Set required environment variables to specify location of environment echo \"export CONDA_ENVS_PATH=\\\"<path_to_environment>\\\"\" >> ~/.bashrc echo \"export CONDA_PKGS_DIRS=\\\"<path_to_packages>\\\"\" >> ~/.bashrc Rerun .bashrc : . ~/.bashrc Create and prepare a conda environment, and activate it with the following commands: # Navigate to <path_to_environment> cd <path_to_environment> # Create conda environment in the current folder conda create --prefix = cid python = 3 # Activate conda environment (can now be done from any folder) conda activate cid # Install necessary packages ## Add conda-forge as the main channel for downloading packages (optional) conda config --add channels conda-forge ## Download required packages conda install -c conda-forge jupyterlab nodejs Install STATA for Jupyter ## Install stata_kernel pip install stata_kernel python -m stata_kernel.install ## Install JupyterLab Extension for Stata syntax highlighting jupyter labextension install jupyterlab-stata-highlight Prepare condor submission and connection scripts (replace <your_token> ) # Make a directory somewhere to house the condor scripts mkdir -p ~/condorscripts/condorlogs && cd ~/condorscripts # Download Jupyter submission script from Github Repo curl -O https://raw.githubusercontent.com/cid-harvard/workshop-cluster-training/master/assets/condorscripts/jupyter.submit # Automatically replace \"~\" in jupyter.submit with the absolute path to your HOME directory sed -i 's@\\~@' \" $HOME \" '@' jupyter.submit # Download Jupyter connection script curl -O https://raw.githubusercontent.com/cid-harvard/workshop-cluster-training/master/assets/condorscripts/condorsshrce.sh # Automatically replace \"username\" with the username sed -i 's/username/' \" $USER \" '/' condorsshrce.sh # Download shell script to activate conda env named 'cid' and run Jupyter curl -O https://raw.githubusercontent.com/cid-harvard/workshop-cluster-training/master/assets/condorscripts/run_jupyter.sh # Replace \"<your_token>\" with token of your choice # Example: sed -i 's/my_token/example_token/' run_jupyter.sh sed -i 's/my_token/<your_token>/' run_jupyter.sh","title":"Setup"},{"location":"cluster_usage/rce_remote_stata_options/#running-stata-through-jupyter","text":"SSH into the cluster, using portforwarding (if you haven't yet) # Replace \"<rce_username>\" with your RCE username ssh -L 8889 :localhost:8889 <rce_username>@rce.hmdc.harvard.edu Submit Jupyter job # Submit condor script condor_submit ~/condorscripts/jupyter.submit Use tmux to handle connection errors/closures (optional) # Start a new tmux window tmux new Connect to the Jupyter server # SSH to the machine running your jupyter server . ~/condorscripts/condorsshrce.sh $USER Tip: tmux keyboard shortcuts and commands New tmux window: tmux new Detach tmux window: ctrl+b , then d Reattach latest tmux window: tmux a New horizontal pane: ctrl+b , then \" New vertical pane: ctrl+b , then % Kill current pane: ctrl+b , then x , then y Move between panes: ctrl+b , then arrow keys Stop moving, or cancel a tmux-specific command: escape Kill all tmux sessions: tmux kill-server In your browser, go to localhost:8889 , and voila! Once you're done, you can close the compute node using ctrl+d , you might then have to press ctrl+c if your login node is taking time to appear Remember to remove the job once you're finished: # Look up running jobs condor_q -global $USER # Remove job condor_rm -name \"<machine_name>\" <ID> Example","title":"Running Stata through Jupyter"},{"location":"cluster_usage/rce_remote_stata_options/#option-3-atom-hydrogen","text":"Note The text editor Atom, using the package Hydrogen, allows you to run code interactively, inspect data and plot using Jupyter kernels. This method uses stata_kernel as well. Recommended option! Provides a do-file like feel , with low latency (i.e. no stuttering).","title":"Option 3: Atom + Hydrogen"},{"location":"cluster_usage/rce_remote_stata_options/#setup_2","text":"","title":"Setup"},{"location":"cluster_usage/rce_remote_stata_options/#setting-up-the-jupyter-kernel","text":"The steps for setting up the Jupyter kernel are the same as for Option 2: Jupyter Lab .","title":"Setting up the Jupyter Kernel"},{"location":"cluster_usage/rce_remote_stata_options/#setting-up-atom","text":"You can learn some Atom basics here Install required packages To install a package on atom, press Cmd+Shift+P (Mac) or Cmd+Shift+P (Windows) to enter the \"Command Palette\", and type Install Packages . Install the following packages: remote-ftp : enable browsing remote files hydrogen : run code through jupyter kernels language-stata : stata code linting Optional (my favourite add-ons): tree-view : explore files in project file-icons : convenient file icons in tree view atom-beautify : automatically indent / beautify code according to linters open-recent : open recently opened files / projects teletype : collaborate on code in real-time (think google docs for code) highlight-selected : highlight all occurrences of selected word or phrase minimap : mini view of the code on the side Configure remote_ftp Create an empty folder in your computer. Call it remote_atom (or any other name you might see fit). Open Atom and click \"File -> Open -> remote_atom \". This will open the remote_atom folder as a \"project\". Open the Command Palette ( Cmd+Shift+P or Ctrl+Shift+P ) and type \"Create Sftp\" and choose the option \"Remote Ftp: Create Sftp Config File\". You will notice that a file named .ftpconfig is automatically created in the same folder. Replace the contents of the file with the following, modifying the user and remote parameters: { \"protocol\" : \"sftp\" , \"host\" : \"rce.hmdc.harvard.edu\" , // string - Hostname or IP address of the server. Default: 'localhost' \"port\" : 22 , // integer - Port number of the server. Default: 22 \"user\" : \"<username>\" , // string - Username for authentication. Default: (none) \"promptForPass\" : true , // boolean - Set to true for enable password/passphrase dialog. This will prevent from using cleartext password/passphrase in this config. Default: false \"remote\" : \"/nfs/home/S/shg309/ <replace with your own home folder>\" , // try to use absolute paths starting with / \"connTimeout\" : 10000 , // integer - How long (in milliseconds) to wait for the SSH handshake to complete. Default: 10000 \"keepalive\" : 10000 // integer - How often (in milliseconds) to send SSH-level keepalive packets to the server (in a similar way as OpenSSH's ServerAliveInterval config option). Set to 0 to disable. Default: 10000 } Open the Command Palette and type \"Remote Ftp\", and choose the option Toggle . This will open up a Remote tab on the left hand side. In the Remote tab, click on Connect , and voila! You can now edit files on the cluster as if they were on your own computer! Warning Don't use the remote_atom project unless you're working on the cluster. The files in the folder are uploaded automatically to the cluster when you reconnect. Note Once you're done, remember to Disconnect . Optional: Through the Command Palette, go to \"View Installed Packages\" --> remote_ftp's settings --> change Auto Upload on Save from always to only when connected Configure Hydrogen Through the Command Palette, go to \"View Installed Packages\" --> hydrogen's settings --> change Kernel Gateways to the following (replace <your_token> with the token you set for Jupyter earlier): [{ \"name\" : \"RCE Jupyter\" , \"options\" :{ \"baseUrl\" : \"http://localhost:8889\" , \"token\" : \"<your_token>\" }}]","title":"Setting up Atom"},{"location":"cluster_usage/rce_remote_stata_options/#running-stata-through-hydrogen","text":"Connect to Remote Kernel Open a Stata .do file Through the Command Palette, go to \"Connect to Remote Kernel\". Troubleshooting If you haven't set a token, you might be asked to \"Authenticate Using Token\". To find the authentication token, on an RCE node, type the following: # Activate conda environment conda activate cid # Get Jupyter token jupyter notebook list The token will be of the form: http://localhost:8889/?token= a61elkjziunns1523e70eb18a452cfsdf9812302199b87e Running code Select code you want to run, and hit Cmd+Enter (Mac) or Ctrl+Enter (Windows) To run all code, hit Cmd+Option+Enter (Mac) or Ctrl+Alt+Enter (Windows) Magics For browsing data, you can use the %browse magic: webuse auto scatter mpg weight %browse 100 mpg Refer to this page for more Jupyter+Stata magics.","title":"Running Stata through Hydrogen"},{"location":"data_management/data_management/","text":"Project Data Management \u00b6 Essentials 1 \u00b6 Backups Cluster data often backed up, but ensure backup frequency is sufficient Solution: external hard drive (local) + cluster backups + CrashPlan (cloud) Above solutions contingent on data agreements / legal restrictions File organization and naming Create a shared system, follow it . Consider date conventions (YYYY-MM-DD), special characters, versioning Documentation (README files) Too much documentation > not enough documentation Document your system aka provide orientation documents Template: Title Data Source with link Added by (CID member's name with email) Dates: - Data time period - Date added - Date modified (if required) Description Variable descriptions (incl keys to join with other data) Limitations Other notes (such as licensing, citation, ethical restrictions, legal restrictions, funder requirements, etc.) Data security All researchers working with Human Subjects are required to get IRB ethics training IRB review where required Don't take data off cluster, especially if sensitive If non-sensitive data has to be moved off of the cluster and into a cloud service, you must encrypt it using VeraCrypt or similar encryption software first. Stata package to help with encryption is available here . Responsibility Assign explicit responsibilities within your project for data management Data Management Checklist \u00b6 Stock-taking: current and future inventory Space requirements, confidentiality and legal requirements Assigned responsibilities for data management Responsibility for enforcement of project data management rules / conventions Storage and backup systems in place Regular backups Off-cluster backups for particularly important data File organization and naming systems in place Data versions - naming and storage conventions Naming, organization and version control for code Access and security guidelines in place Tightly controlled access Understand responsibilities with access Is my data confidential ? Data agreements and legal restrictions Rules, conventions documented Rules and conventions laid out in easily accessible document Handed to new joinees as part of their orientation (both at org and project level) Ethics and privacy concerns addressed Project IRB review IRB ethics certifications Projects involving existing non-public data are usually required to be reviewed by IRB Resources \u00b6 Read the detailed version of the above checklist adapted from MIT Libraries' resources Another good checklist available at page 17 (Appendix A) of ICPSR's booklet Tool available for Data Management Planning: DMPTool . Note that you don't have to \"submit\" the data management plan to a funder through DMPTool unless you're explicitly asked to by the funder. J-PAL Data Management guidelines: https://www.povertyactionlab.org/research-resources/working-with-data Adapted from MIT Libraries' resources on data management released under a CC-BY license \u21a9","title":"Overview"},{"location":"data_management/data_management/#project-data-management","text":"","title":"Project Data Management"},{"location":"data_management/data_management/#essentials1","text":"Backups Cluster data often backed up, but ensure backup frequency is sufficient Solution: external hard drive (local) + cluster backups + CrashPlan (cloud) Above solutions contingent on data agreements / legal restrictions File organization and naming Create a shared system, follow it . Consider date conventions (YYYY-MM-DD), special characters, versioning Documentation (README files) Too much documentation > not enough documentation Document your system aka provide orientation documents Template: Title Data Source with link Added by (CID member's name with email) Dates: - Data time period - Date added - Date modified (if required) Description Variable descriptions (incl keys to join with other data) Limitations Other notes (such as licensing, citation, ethical restrictions, legal restrictions, funder requirements, etc.) Data security All researchers working with Human Subjects are required to get IRB ethics training IRB review where required Don't take data off cluster, especially if sensitive If non-sensitive data has to be moved off of the cluster and into a cloud service, you must encrypt it using VeraCrypt or similar encryption software first. Stata package to help with encryption is available here . Responsibility Assign explicit responsibilities within your project for data management","title":"Essentials1"},{"location":"data_management/data_management/#data-management-checklist","text":"Stock-taking: current and future inventory Space requirements, confidentiality and legal requirements Assigned responsibilities for data management Responsibility for enforcement of project data management rules / conventions Storage and backup systems in place Regular backups Off-cluster backups for particularly important data File organization and naming systems in place Data versions - naming and storage conventions Naming, organization and version control for code Access and security guidelines in place Tightly controlled access Understand responsibilities with access Is my data confidential ? Data agreements and legal restrictions Rules, conventions documented Rules and conventions laid out in easily accessible document Handed to new joinees as part of their orientation (both at org and project level) Ethics and privacy concerns addressed Project IRB review IRB ethics certifications Projects involving existing non-public data are usually required to be reviewed by IRB","title":"Data Management Checklist"},{"location":"data_management/data_management/#resources","text":"Read the detailed version of the above checklist adapted from MIT Libraries' resources Another good checklist available at page 17 (Appendix A) of ICPSR's booklet Tool available for Data Management Planning: DMPTool . Note that you don't have to \"submit\" the data management plan to a funder through DMPTool unless you're explicitly asked to by the funder. J-PAL Data Management guidelines: https://www.povertyactionlab.org/research-resources/working-with-data Adapted from MIT Libraries' resources on data management released under a CC-BY license \u21a9","title":"Resources"},{"location":"data_management/folder_structure/","text":"Folder Structure \u00b6 Overall Structure \u00b6 data/ data_1/ raw/ ---- Raw, immutable files file_1 README.md ---- Describes file_1: sources, quirks, codebook processed/ ---- Cleaned, reshaped, filtered files file_1 README.md ---- Describes file_1, the cleaned version. Include codebook if required user_1/ src/ ---- Scripts / do-files are stored here proj/ ---- Notebooks, experimental do-files stored here figs/ tables/ documents/ Example folder structure \u00b6 data/ nightlights/ raw/ luminosity_lksjdf111.csv intermediate/ processed/ luminosity.pq social_security/ raw/ incomes_gibberish.csv intermediate/ processed/ incomes.pq dario/ .git/ .gitignore darios_personal_files.secret src/ preprocessing/ cleaning_master.py cleaning_nightlights.py cleaning_social_security.py commonly_used_functions.py bash_scripts.sh notebooks/ 0-run_cleaning.ipynb 1-modelling.ipynb figs/ tables/ documents/ andres/ .git/ .gitignore src/ preprocessing/ cleaning_master.py cleaning_nightlights.py cleaning_social_security.py commonly_used_functions.py bash_scripts.sh notebooks/ 0-run_cleaning.ipynb 1-modelling.ipynb figs/ tables/ documents/ Notes \u00b6 Data folders: README.md or README.txt files in every raw and processed folders should reference every file in the folders Raw files are immutable - don't touch them once you've downloaded them Intermediate files can be altered freely Processed files can be altered, as long as you first consider the repercussions on subsequent analyses Files are cleaned, reshaped, filtered from raw to intermediate (optional), and finally to processed stages. All analyses have to be based on files in the processed folder ONLY.","title":"Folder structures"},{"location":"data_management/folder_structure/#folder-structure","text":"","title":"Folder Structure"},{"location":"data_management/folder_structure/#overall-structure","text":"data/ data_1/ raw/ ---- Raw, immutable files file_1 README.md ---- Describes file_1: sources, quirks, codebook processed/ ---- Cleaned, reshaped, filtered files file_1 README.md ---- Describes file_1, the cleaned version. Include codebook if required user_1/ src/ ---- Scripts / do-files are stored here proj/ ---- Notebooks, experimental do-files stored here figs/ tables/ documents/","title":"Overall Structure"},{"location":"data_management/folder_structure/#example-folder-structure","text":"data/ nightlights/ raw/ luminosity_lksjdf111.csv intermediate/ processed/ luminosity.pq social_security/ raw/ incomes_gibberish.csv intermediate/ processed/ incomes.pq dario/ .git/ .gitignore darios_personal_files.secret src/ preprocessing/ cleaning_master.py cleaning_nightlights.py cleaning_social_security.py commonly_used_functions.py bash_scripts.sh notebooks/ 0-run_cleaning.ipynb 1-modelling.ipynb figs/ tables/ documents/ andres/ .git/ .gitignore src/ preprocessing/ cleaning_master.py cleaning_nightlights.py cleaning_social_security.py commonly_used_functions.py bash_scripts.sh notebooks/ 0-run_cleaning.ipynb 1-modelling.ipynb figs/ tables/ documents/","title":"Example folder structure"},{"location":"data_management/folder_structure/#notes","text":"Data folders: README.md or README.txt files in every raw and processed folders should reference every file in the folders Raw files are immutable - don't touch them once you've downloaded them Intermediate files can be altered freely Processed files can be altered, as long as you first consider the repercussions on subsequent analyses Files are cleaned, reshaped, filtered from raw to intermediate (optional), and finally to processed stages. All analyses have to be based on files in the processed folder ONLY.","title":"Notes"}]}